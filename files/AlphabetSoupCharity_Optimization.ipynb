{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd\n",
    "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df = application_df.drop(columns=['EIN', 'NAME']) # Remove columns \"EIN\" and \"NAME\" from the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "unique_values = application_df.nunique() # Count unique values in each column \n",
    "\n",
    "# Display the count of unique values \n",
    "unique_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at APPLICATION_TYPE value counts to identify and replace with \"Other\"\n",
    "application_type_count = application_df['APPLICATION_TYPE'].value_counts() # Counts the occurrences of each application type \n",
    "\n",
    "# Display the value counts\n",
    "application_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace = application_type_count[application_type_count < 500].index.tolist() # Replace types wih fewer than 500 occurrences\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure replacement was successful\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at CLASSIFICATION value counts to identify and replace with \"Other\"\n",
    "classification_counts = application_df['CLASSIFICATION'].value_counts() # Count occurrences of each classification\n",
    "\n",
    "# Display the vlaue counts\n",
    "classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "classification_counts_g1 = classification_counts[classification_counts > 1] # Filter classifications with counts greater than 1 \n",
    "\n",
    "# Display value counts\n",
    "classification_counts_g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classification_to_replace = classification_counts[classification_counts < 100].index.tolist() # Replace classifications with fewer than 100 occurrences\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classification_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "\n",
    "# Check to make sure replacement was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "application_df = pd.get_dummies(application_df, drop_first=True) # Perform one-hot encoding and drop the first category \n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "application_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "x = application_df.drop(columns=[\"IS_SUCCESSFUL\"]) # Features all except the target \n",
    "y = application_df[\"IS_SUCCESSFUL\"] # Target is the IS_SUCCESSFUL column \n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) # 80% training, 20% testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "x_scaler = scaler.fit(x_train)\n",
    "\n",
    "# Scale the data\n",
    "x_train_scaled = x_scaler.transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "input_features = x_train.shape[1] # Number of input features (columns in the training dataset)\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=80, activation='relu', input_dim=input_features)) # 80 nerons, ReLU activation, input layer specified \n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=30, activation='relu')) # 30 nerons, ReLU activation \n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) # Single neron, Sigmoid activation for binary classification  \n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Increase neurons in the hidden layers\n",
    "# Updated architecture for better learning capacity\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer with more neurons\n",
    "nn.add(tf.keras.layers.Dense(units=100, activation='relu', input_dim=input_features))\n",
    "\n",
    "# Second hidden layer with more neurons\n",
    "nn.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Add a third hidden layer\n",
    "# Introduced additional complexity to capture deeper patterns\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=100, activation='relu', input_dim=input_features))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=25, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Add Dropout to reduce overfitting\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer with Dropout\n",
    "nn.add(tf.keras.layers.Dense(units=100, activation='relu', input_dim=input_features))\n",
    "nn.add(Dropout(0.2))  # Drop 20% of neurons\n",
    "\n",
    "# Second hidden layer with Dropout\n",
    "nn.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(optimizer='adam', # Optimizer to adjust weights based on loss function\n",
    "           loss='binary_crossentropy', # Loss function for binary classification\n",
    "           metrics=['accuracy']) # Metric to add during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = nn.fit(x_train_scaled, y_train, # Training data\n",
    "                 epochs=50, # Number of times the model sees the full dataset\n",
    "                 batch_size=32, # Numbers of samples per update\n",
    "                 validation_data=(x_test_scaled, y_test)) # Validation data to monitor accuracy during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Train with more epochs\n",
    "# Increased epochs from 50 to 100 to improve learning over time\n",
    "history = nn.fit(x_train_scaled, y_train, \n",
    "                 epochs=100, \n",
    "                 batch_size=32, \n",
    "                 validation_data=(x_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Train with smaller batch size\n",
    "# Reduced batch size to 16 for finer weight updates\n",
    "history = nn.fit(x_train_scaled, y_train, \n",
    "                 epochs=50, \n",
    "                 batch_size=16, \n",
    "                 validation_data=(x_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(x_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn.save(\"AlphabetSoupCharity.h5\") # Save the trained model to a file name 'AlphabetSoupCharity.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Despite multiple attempts to optimize the neural network, the highest accuracy achieved was **73.2%**, which fell short of the target of **75%**. Here is a brief overview of what was tried and its outcomes:\n",
    "\n",
    "### Increased Neurons in Hidden Layers\n",
    "- Added more neurons (100 in the first layer, 50 in the second).\n",
    "- **Result**: Improved accuracy slightly to **73.0%**.\n",
    "\n",
    "### Added a Third Hidden Layer\n",
    "- Introduced a third hidden layer with 25 neurons to increase model complexity.\n",
    "- **Result**: Accuracy dropped to **72.6%**, possibly due to overfitting.\n",
    "\n",
    "### Trained with More Epochs\n",
    "- Increased the number of training epochs from 50 to 100.\n",
    "- **Result**: Accuracy decreased slightly, likely due to overfitting the training data.\n",
    "\n",
    "### Reduced Batch Size\n",
    "- Reduced batch size to 16 for finer weight updates during training.\n",
    "- **Result**: Accuracy improved slightly to **73.1%**.\n",
    "\n",
    "### Added Dropout\n",
    "- Added dropout layers to the model to reduce overfitting by randomly dropping 20% of neurons during training.\n",
    "- **Result**: Accuracy dropped significantly to **57%**.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Even after trying different ways to improve the model, the highest accuracy I could get was **73.2%**, which is below the **75%** target. I made several changes, including adding more neurons, using more training epochs, changing the batch size, and grouping rare categories. Some of these changes helped a little, but none were enough to reach the goal.\n",
    "\n",
    "This shows how tricky it can be to improve a model when working with a dataset like this. Even though the target wasn’t reached, the process helped me understand how adjustments affect the model’s performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
